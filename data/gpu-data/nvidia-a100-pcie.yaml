# yaml-language-server: $schema=./gpu-spec.schema.json
name: nvidia-a100-pcie
label: NVIDIA A100 PCIe
tensorCoreCount: 432
fp32TFLOPS: 156
fp16TFLOPS: 312
int8TOPS: 624
memoryCapacityGB: 80
memoryBandwidthGBs: 1935
gpuArchitecture: ampere
supportedHardwareOperations:
  - FP16
  - INT8
  - INT4
  - INT1
  - FP64
  - TF32
  - BF16
supportedCUDAComputeCapability: 8
releaseDate: "2021-06-28"
lastModified: "2025-12-05T00:00:00Z"

summary: The NVIDIA A100 PCIe 80GB was announced in May 2020, is a formidable
  accelerator in the field of machine learning and artificial intelligence.
  Built on NVIDIA's advanced Ampere architecture, this accelerator is designed
  for high-performance computing, deep learning training, and inference tasks.
  With its massive 80 GB of HBM2e memory and superior memory bandwidth of 1,935
  GB/s, it caters to the most demanding AI workloads. The inclusion of 432
  tensor cores significantly accelerates machine learning applications, making
  it a go-to choice for researchers and data scientists. Operating at a base
  clock of 1065 MHz and a boost clock up to 1410 MHz, it delivers impressive
  computational power, capped at a maximum power consumption of 300 watts. The
  A100-PCIE-80GB is notable for its high FP32 performance of 19.5 TFLOPS,
  emphasizing its capability in handling floating-point operations efficiently.
references:
  - https://www.nvidia.com/en-us/data-center/a100/
  - https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-nvidia-us-2188504-web.pdf
  - https://developer.nvidia.com/blog/nvidia-ampere-architecture-in-depth/
  - https://www.techpowerup.com/gpu-specs/a100-pcie-80-gb.c3821
  - https://www.nvidia.com/en-us/data-center/tensor-cores/
notes:
  - "fp32TFLOPS: NOTE: This is the Tensor Core FP32 performance, not the CUDA
    FP32 performance. The A40 for example doesn't have such a spec (only CUDA)."
