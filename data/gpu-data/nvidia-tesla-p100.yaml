name: nvidia-tesla-p100
label: NVIDIA Tesla P100
fp32TFLOPS: 10.6
fp16TFLOPS: 21.2
memoryCapacityGB: 16
memoryBandwidthGBs: 732
maxTDPWatts: 300
gpuArchitecture: pascal
supportedHardwareOperations:
  - FP16
  - FP32
supportedCUDAComputeCapability: 6
summary: The NVIDIA Tesla P100 was announced in 2016 as the first of their
  Pascal architecture powered Tesla cards. The Tesla P100 came in PCIe and SXM
  versions and was the first GPU accelerator to use High Bandwidth Memory 2
  (HBM2). It did not have Tensor Cores, so it isn't usually preferred for
  machine learning. However, it's FP32 and FP16 FLOPs are still quite good.
references:
  - https://www.anandtech.com/show/10433/nvidia-announces-pci-express-tesla-p100
  - https://developer.nvidia.com/blog/mixed-precision-programming-cuda-8/
  - https://www.anandtech.com/show/10222/nvidia-announces-tesla-p100-accelerator-pascal-power-for-hpc
notes:
  - "tensorCoreCount: Tesla P100 only has CUDA cores"
  - 'supportedHardwareOperations: It appears that hardware-accelerated GEMM
    operations require "Warp matrix functions" supported in Compute Capability
    >=7.x:
    https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities'
  - "supportedHardwareOperations: The 8-bit and 16-bit DP4A and DP2A dot product
    instructions are supported on GP102-GP106, but not on GP100."
