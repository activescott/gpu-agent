# yaml-language-server: $schema=./gpu-spec.schema.json
name: nvidia-a30
label: NVIDIA A30
tensorCoreCount: 224
fp32TFLOPS: 165
fp16TFLOPS: 165
int8TOPS: 330
memoryCapacityGB: 24
memoryBandwidthGBs: 933.3
gpuArchitecture: ampere
supportedHardwareOperations:
  - FP16
  - INT8
  - INT4
  - INT1
  - FP64
  - TF32
  - BF16
supportedCUDAComputeCapability: 8
releaseDate: "2021-04-12"
lastModified: "2025-12-05T00:00:00Z"

summary: The NVIDIA A30, launched on April 12th, 2021 with part number
  900-21001-0040-100, is a professional-grade accelerator geared towards machine
  learning and AI computation. Built on the GA100 graphics processor and
  utilizing a 7 nm process, it excels in handling large-scale AI and machine
  learning tasks. The A30 is notable for its high memory bandwidth and
  substantial tensor core count, which significantly boosts the speed of machine
  learning applications. Its optimized power consumption also makes it a
  preferred choice for sustainable, high-efficiency data center deployments.
references:
  - https://www.nvidia.com/en-us/data-center/products/a30-gpu/
  - https://www.techpowerup.com/gpu-specs/a30-pcie.c3792
  - https://www.nvidia.com/en-us/data-center/tensor-cores/
notes:
  - 'fp32TFLOPS: NOTE: This is the Tensor Core FP32 performance, not the CUDA
    FP32 performance. Noted in the data sheet as "NVIDIA A30 delivers 165
    teraFLOPS (TFLOPS) of TF32 deep learning performance."'
  - "fp16TFLOPS: NOTE: This is the Tensor Core FP16 performance, not the CUDA
    FP16 performance."
