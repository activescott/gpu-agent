import Link from "next/link"

# Technical Specifications Relevant for Machine Learning GPUs & Accelerators

When evaluating GPUs for training or inference of large deep learning models, some crucial key technical specifications should be considered:

- **Tensor Cores**: Tensor cores are specialized chips that efficiently perform matrix multiplication, the fundamental operation in deep learning. Since deep learning workloads are generally highly paralizable, generally more tensor cores will generally improve the performance of deep learning workloads.
- **Memory Capacity**: Amount of onboard memory, usually measured in GB. Larger models require more memory for both training and inference.
- **Memory Bandwidth**: The rate at which data can be read from or stored into a memory by the GPU is measured in GB/s. In many cases Tensor Cores are idel waiting on matricies to be moved in and out of memory.

Deep learning models can be trained and deployed using different precision levels, such as single-precision (FP32), half-precision (FP16), or mixed precision (FP32/FP16). Each GPU has different performance for different precision levels which can have a dramatic impact training and inference performance.

- **FP32 Performance**: 32-bit floating point performance indicates the GPU's speed of processing 32-bit floating-point operations. Measured in teraflops (TFLOPS).
- **FP16 Performance**: 16-bit floating point performance indicates the GPU's speed of processing 16-bit floating-point operations. Measured in teraflops (TFLOPS).
- **INT8 Performance**: 8-bit integer performance indicates the GPU's speed of processing 8-bit integer operations. Measured in tera-operations per second (TOPS).
- **Max Power Consumption**: The maximum amount of power consumed by the card, measured in watts.

## Which Percision is Best?

The most commonly used precision levels in state-of-the-art (SOTA) deep learning models are FP32 and FP16. FP32 (single-precision) is the standard precision format for deep learning, offering high accuracy but also requiring significant computational resources and memory bandwidth. FP16 (half-precision) offers a balance between accuracy and efficiency, reducing memory requirements and computational costs by using 16-bit floating-point numbers instead of 32-bit.

Some models will degrade in accuracy dramatically when using lower precision and some won't. Generally you'll have to look at the particular type of model you're training or running inference with to see which percisions are relevant.

However, techniques are evolving to use lower precisions effectively the rapidly evolving space of Quantiziation. Read more at <Link href="/ml/learn/quantization">Quantization in Machine Learning and Deep Learning</Link>.
