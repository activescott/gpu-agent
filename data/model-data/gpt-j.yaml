# yaml-language-server: $schema=./model-spec.schema.json
name: gpt-j
label: GPT-J
modelType: llm
summary: |
  GPT-J is an open-source 6 billion parameter generative pre-trained transformer model developed by EleutherAI. As a GPT-3-inspired architecture, it includes distinctive features like Rotary Position Embeddings and dense attention, contributing to its efficiency in natural language processing tasks. It was trained on the Pile dataset using the Mesh Transformer JAX library and shows commendable performance in code generation and text continuation tasks.
useCase: Text generation, code generation, natural language processing tasks
creator:
  organization: EleutherAI
modelCardLink: https://huggingface.co/EleutherAI/gpt-j-6b
modelArchitecture: Generative Pre-trained Transformer with Rotary Position Embeddings and dense attention
parameterCount: "6B"
gpuMemoryRequirementGB: 12
quantizationVersions:
  - name: FP16
    memoryRequirementGB: 12
  - name: INT8
    memoryRequirementGB: 6
  - name: INT4
    memoryRequirementGB: 3
releaseDate: "2021-06-04"
license: Apache 2.0
paperUrl: null
trainingData: The Pile dataset - a diverse 825GB language modeling dataset
evaluationBenchmarks:
  - LAMBADA
  - HellaSwag
  - WinoGrande
  - ARC
  - PIQA
contextLength: 2048
huggingfaceModelId: EleutherAI/gpt-j-6b
updatedAt: "2025-12-04T19:00:00Z"
references:
  - https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/
  - https://github.com/kingoflolz/mesh-transformer-jax
  - https://huggingface.co/EleutherAI/gpt-j-6b
  - https://arxiv.org/abs/2101.00027
  - https://en.wikipedia.org/wiki/GPT-J
notes:
  - Model is not designed for factual accuracy, only probabilistic text generation
  - Fine-tuning recommended for specific tasks
  - Potential biases present from training data
  - Performance comparable to GPT-3 of similar size in various tasks
  - GPT-J does not have a dedicated research paper; primary documentation is creator Aran Komatsuzaki's blog post and the mesh-transformer-jax GitHub repository
  - Training dataset documented in The Pile paper (arXiv:2101.00027)
  - Developed by Ben Wang and Aran Komatsuzaki at EleutherAI
